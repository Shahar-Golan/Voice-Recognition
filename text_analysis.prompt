podcast_text_topics.prompt
--------------------------

## 0. Text topic analysis overview

Goal:
Analyze the transcript of the podcast (already generated by ASR) to discover topic structure, using only NLP libraries (no LLM):

- Preprocess transcript text
- Group segments into larger text blocks (time-based or token-based)
- Compute semantic embeddings for each block (sentence-transformers)
- Compute cosine similarity between adjacent blocks
- Use low similarity to detect potential topic changes
- Cluster blocks into topics using unsupervised clustering
- Extract keywords per topic using TF-IDF
- Produce a final topic timeline: segments labeled by topic + keywords

Assumptions:
- You already have a transcript with timestamps from the audio pipeline:
  - outputs/audio_features/transcript_segments_for_audio.json

-----------------------------------------------------
## 1. Environment and project skeleton (text part)
-----------------------------------------------------

[ ] 1.1. Project structure (text-related parts)

Within podcast_analysis/:

- outputs/
  - text_analysis/
    - transcript_plain.txt
    - text_blocks.json
    - block_embeddings.npy
    - block_similarities_and_boundaries.csv
    - block_topics.csv
    - topic_keywords.json
    - final_topic_segments.json
- src/
  - text_blocks.py
  - embeddings_topics.py
  - run_text_pipeline.py

[ ] 1.2. Python dependencies for text analysis

Install:

- spacy
- sentence-transformers
- scikit-learn
- pandas
- numpy

Also:
- Download spaCy model, e.g.:
  - python -m spacy download en_core_web_sm

[ ] 1.3. Extend config.py with text constants

Examples:

- TRANSCRIPT_JSON_FOR_TEXT = "outputs/audio_features/transcript_segments_for_audio.json"
- TRANSCRIPT_PLAIN_PATH = "outputs/text_analysis/transcript_plain.txt"
- BLOCKS_JSON_PATH = "outputs/text_analysis/text_blocks.json"
- BLOCK_EMBEDDINGS_PATH = "outputs/text_analysis/block_embeddings.npy"
- BLOCK_SIM_BOUNDARIES_PATH = "outputs/text_analysis/block_similarities_and_boundaries.csv"
- BLOCK_TOPICS_CSV_PATH = "outputs/text_analysis/block_topics.csv"
- TOPIC_KEYWORDS_JSON_PATH = "outputs/text_analysis/topic_keywords.json"
- FINAL_TOPIC_SEGMENTS_JSON_PATH = "outputs/text_analysis/final_topic_segments.json"
- BLOCK_DURATION_SEC = 120.0  # example: 2-minute blocks
- TOPIC_CLUSTER_COUNT = 6      # example: initial guess for number of topics
- ADJACENT_SIM_THRESHOLD = 0.7 # example threshold for topic boundary

-----------------------------------------
## 2. Build plain-text transcript from JSON
-----------------------------------------

[ ] 2.1. Load transcript_segments_for_audio.json

Tasks (in text_blocks.py):
- Read JSON list of segments:
  - Each segment: { "start": float, "end": float, "text": string }

[ ] 2.2. Create plain text transcript file

Tasks:
- Concatenate all segment["text"] lines into one long text string.
- Optionally insert newline between segments for readability.
- Save to:
  - outputs/text_analysis/transcript_plain.txt

This file is primarily for manual inspection and for some global analyses; the block-building step will use the segment-level data to preserve timing.

-------------------------------------------------
## 3. Build text blocks with time information
-------------------------------------------------

[ ] 3.1. Decide block strategy

Option A: time-based blocks
- Each block covers a fixed duration, e.g., BLOCK_DURATION_SEC = 120.0 seconds.

Option B: token-based blocks
- Each block aims for approx N tokens (e.g., 300–500 tokens).
- For simplicity, start with time-based.

[ ] 3.2. Implement block construction in text_blocks.py

Tasks:
- Load transcript_segments_for_audio.json
- Iterate segments in chronological order.
- Maintain current block:
  - block_id (starting from 0)
  - current_block_start_sec (start of first segment assigned)
  - current_block_end_sec (updated as you add segments)
  - current_block_text (concatenation of segment["text"])
- For each segment:
  - If adding this segment keeps block duration <= BLOCK_DURATION_SEC:
    - Add segment text to current_block_text.
    - Update current_block_end_sec = segment["end"].
  - Else:
    - Save current block to a list.
    - Start a new block with this segment.

Block structure example:
- {
    "block_id": int,
    "start_sec": float,
    "end_sec": float,
    "text": string
  }

[ ] 3.3. Save text blocks

- Save blocks list as JSON:
  - outputs/text_analysis/text_blocks.json

[ ] 3.4. Sanity checks

- Print number of blocks.
- Print first 1–2 blocks (start_sec, end_sec, trimmed text).
- Ensure last block end_sec ~ last segment end time.

-------------------------------------------
## 4. Preprocess text for embeddings/TF-IDF
-------------------------------------------

[ ] 4.1. Implement basic text cleaning in text_blocks.py

Tasks:
- For each block["text"]:
  - Lowercase
  - Strip extra whitespace
  - Optionally remove filler words (e.g., "uh", "um") via simple replacement
- Store cleaned text as:
  - "clean_text" field in each block dict
    OR create a separate list aligned by block_id.

[ ] 4.2. Save or return cleaned texts

Options:
- Return list of cleaned texts from a helper function.
- Or extend text_blocks.json to have "clean_text" per block.

-----------------------------------------------
## 5. Compute semantic embeddings for each block
-----------------------------------------------

[ ] 5.1. Load sentence-transformers model in embeddings_topics.py

Tasks:
- from sentence_transformers import SentenceTransformer
- model_name example: "all-MiniLM-L6-v2"
- model = SentenceTransformer(model_name)

[ ] 5.2. Generate embeddings

Tasks:
- Load blocks from text_blocks.json.
- Extract clean_text for each block into a list texts.
- embeddings = model.encode(texts, batch_size=some_value, show_progress_bar=True)
- embeddings shape: (num_blocks, embedding_dim)

[ ] 5.3. Save embeddings

- Save numpy array:
  - np.save(BLOCK_EMBEDDINGS_PATH, embeddings)
- Keep block_id alignment:
  - row i of embeddings corresponds to block_id i.

[ ] 5.4. Sanity check

- Print embeddings.shape
- Print the norm (e.g., np.linalg.norm(embeddings[0])).

----------------------------------------------------------
## 6. Compute cosine similarity between adjacent blocks
----------------------------------------------------------

[ ] 6.1. Load embeddings and compute similarities

Tasks:
- Load embeddings from BLOCK_EMBEDDINGS_PATH
- For i from 0 to num_blocks-2:
  - Compute cosine similarity between embeddings[i] and embeddings[i+1]
  - Use sklearn.metrics.pairwise.cosine_similarity or manual formula.

Store results in a list of dicts:
- {
    "block_id": i,
    "next_block_id": i+1,
    "cosine_sim": float
  }

[ ] 6.2. Decide boundary rule

Tasks:
- Use ADJACENT_SIM_THRESHOLD from config, e.g., 0.7
- For each pair:
  - is_boundary = (cosine_sim < ADJACENT_SIM_THRESHOLD)
- Add is_boundary flag to each record.

[ ] 6.3. Save similarities and boundaries

- Convert to pandas DataFrame with columns:
  - block_id, next_block_id, cosine_sim, is_boundary
- Save as CSV:
  - outputs/text_analysis/block_similarities_and_boundaries.csv

[ ] 6.4. Optional analysis

- Plot histogram of cosine_sim to see distribution.
- Adjust ADJACENT_SIM_THRESHOLD if too few or too many boundaries.

-------------------------------------------
## 7. Cluster blocks into topic groups
-------------------------------------------

[ ] 7.1. Choose and configure clustering algorithm

Options:
- KMeans (requires specifying number of clusters)
- AgglomerativeClustering (hierarchical)

Tasks:
- For initial version, use KMeans with:
  - n_clusters = TOPIC_CLUSTER_COUNT from config.

[ ] 7.2. Run clustering in embeddings_topics.py

Tasks:
- Load embeddings array.
- Fit KMeans:
  - from sklearn.cluster import KMeans
  - kmeans = KMeans(n_clusters=TOPIC_CLUSTER_COUNT, random_state=0)
  - kmeans.fit(embeddings)
- labels = kmeans.labels_

[ ] 7.3. Create block_topics DataFrame

Columns:
- block_id
- start_sec
- end_sec
- topic_id = labels[block_id]

Use blocks from text_blocks.json to fill start_sec and end_sec.

[ ] 7.4. Save block topic assignments

- Save DataFrame as:
  - outputs/text_analysis/block_topics.csv

[ ] 7.5. Optional cluster quality checks

- Count how many blocks per topic.
- Compute cluster centers and inspect nearest blocks per topic for sanity.

-----------------------------------------------------
## 8. Extract keywords for each topic (no LLM, TF-IDF)
-----------------------------------------------------

[ ] 8.1. Prepare texts grouped by topic

Tasks:
- Load block_topics.csv and text_blocks.json.
- For each topic_id:
  - Collect clean_text of all blocks with that topic_id.
  - Combine texts into one large document per topic.

This gives a list:
- topic_docs[topic_id] = big string with all text for that topic.

[ ] 8.2. Compute TF-IDF and extract top terms

Tasks:
- from sklearn.feature_extraction.text import TfidfVectorizer
- vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
- X = vectorizer.fit_transform(topic_docs)
- For each topic_id:
  - Get row X[topic_id]
  - Sort terms by TF-IDF weight
  - Extract top N (e.g., 5–10) keywords

Store structure:
- {
    "topic_id": int,
    "keywords": [list of top words]
  }

[ ] 8.3. Save topic_keywords.json

- Save list of objects, one per topic:
  - outputs/text_analysis/topic_keywords.json

Check:
- For each topic_id, print keywords to verify they are meaningful.

------------------------------------------------------
## 9. Build final topic segments (timeline of the podcast)
------------------------------------------------------

[ ] 9.1. Combine clustering + boundaries

Goal:
Create final topic segments along the time axis.

Option A (simple, clustering only):
- A topic segment is any consecutive run of blocks with same topic_id.

Option B (hybrid, clustering + boundaries):
- Treat is_boundary=True from adjacent similarities as forced breakpoints.
- So even if topic_id is same, you can start a new segment after a strong boundary.

Start with Option A for simplicity.

[ ] 9.2. Construct segments from block_topics

Tasks:
- Load block_topics.csv (with block_id, start_sec, end_sec, topic_id).
- Iterate blocks in ascending block_id order.
- Keep current segment:
  - segment_id
  - topic_id
  - start_sec
  - end_sec (updated)
  - block_ids in segment
- For each block i:
  - If this is the first block, start a new segment.
  - Else:
    - If block.topic_id == current_segment.topic_id:
      - Extend current segment: update end_sec, append block_id.
    - Else:
      - Close current segment and store it.
      - Start new segment with block i.

Segment structure:
- {
    "segment_id": int,
    "topic_id": int,
    "start_sec": float,
    "end_sec": float,
    "block_ids": [list of ints]
  }

[ ] 9.3. Attach keywords to segments

Tasks:
- Load topic_keywords.json.
- For each segment:
  - Find keywords for its topic_id.
  - Add "keywords" field to segment dict.

Final segment structure:
- {
    "segment_id": int,
    "topic_id": int,
    "start_sec": float,
    "end_sec": float,
    "block_ids": [...],
    "keywords": ["election", "california", ...]
  }

[ ] 9.4. Save final_topic_segments.json

- Save list of segments as:
  - outputs/text_analysis/final_topic_segments.json

Check:
- Print each segment with:
  - time range in minutes (start_sec/60, end_sec/60)
  - topic_id
  - keywords

-----------------------------------------
## 10. Run the text topic pipeline end-to-end
-----------------------------------------

[ ] 10.1. Implement run_text_pipeline.py

Steps:
- Step 1: Load transcript_segments_for_audio.json
- Step 2: Build and save plain-text transcript
- Step 3: Build text blocks with time info
- Step 4: Clean block texts
- Step 5: Compute block embeddings and save
- Step 6: Compute adjacent cosine similarities and boundaries (optional but useful)
- Step 7: Cluster blocks into topics and save block_topics.csv
- Step 8: Compute topic-level TF-IDF keywords and save topic_keywords.json
- Step 9: Build final topic segments (timeline) and save final_topic_segments.json

[ ] 10.2. Manual inspection and refinement

Tasks:
- Review topic_keywords.json to ensure topics make sense.
- Review final_topic_segments.json and compare to transcript_plain.txt:
  - Check if time ranges align with intuitive topic changes.
- Adjust parameters as needed:
  - BLOCK_DURATION_SEC
  - TOPIC_CLUSTER_COUNT
  - ADJACENT_SIM_THRESHOLD (if used in hybrid segmentation strategy)

[ ] 10.3. Document the topic analysis

Add to README:
- How to run run_text_pipeline.py
- Explanation of each output file in outputs/text_analysis/
- Example interpretation of topics and segments for your podcast.
